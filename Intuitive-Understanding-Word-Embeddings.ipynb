{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直觉理解词嵌入：从计数向量到Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文地址： https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "可视化word embedding visual inspector https://ronxin.github.io/wevi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "## 引言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, have a look at the below examples.\n",
    "\n",
    "1. You open Google and search for a news article on the ongoing Champions trophy and get hundreds of search results in return about it.\n",
    "2. Nate silver analysed millions of tweets and correctly predicted the results of 49 out of 50 states in 2008 U.S Presidential Elections.\n",
    "3. You type a sentence in google translate in English and get an Equivalent Chinese conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们开始之前，看看如下例子：\n",
    "1. 你打开Google,搜索关于正在进行的冠军杯的新闻文章，返回数百个搜索结果。\n",
    "2. Nate silver分析数十亿tweets，2008年美国总统竞选，正确预测了确预测中了49个州，共50个州。\n",
    "3. 你在谷歌翻译中用英文输入一个语句，然后获得一个相同意思的中文对话。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do the above examples have in common?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，上面的例子有什么共同特点呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You possible guessed it right – **TEXT processing**. All the above three scenarios deal with humongous amount of text to perform different range of tasks like clustering in the google search example, classification in the second and Machine Translation in the third."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能猜对它——**文本处理**。上面三种场景处理巨大数量的文本来执行不同任务，如谷歌搜索终的聚类，第二个情景中的分类，第三个情景中的机器翻译。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humans can deal with text format quite intuitively but provided we have millions of documents being generated in a single day, we cannot have humans performing the above the three tasks. It is neither scalable nor effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人类可以直觉地处理文本格式，但是提供在一天中产生的数百万文档，不管是在扩展能力还是效率上,对于上面这三种任务我们无能为力执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we make computers of today perform clustering, classification etc on a text data since we know that they are generally inefficient at handling and processing strings or texts for any fruitful outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，我们如何让今天的计算机执行文本信息聚类、分类等，因为我们知道他们通常低效处理字符串或者文本获得有效输出？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, a computer can match two strings and tell you whether they are same or not. But how do we make computers tell you about football or Ronaldo when you search for Messi? How do you make a computer understand that “Apple” in “Apple is a tasty fruit” is a fruit that can be eaten and not a company?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "肯定的，一台计算机可以匹配两个字符串然后告诉你它们是否是相同的。但是我们如何使计算机在你搜索梅西时告诉你足球和罗纳尔多呢？如何使计算机理解“苹果是一种美味的水果”中的“苹果”是一种可以吃的水果而非一个公司呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer to the above questions lie in creating a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述问题的答案依赖于创建一种可以捕获他们的意思、语义关系和他们所使用的不同上下文类型的词语表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all of these are implemented by using Word Embeddings or numerical representations of texts so that computers may handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有这些使用词嵌入或文本的数字表示，使得计算机可以处理它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will see formally what are Word Embeddings and their different types and how we can actually implement them to perform the tasks like returning efficient Google search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们将正式的看到什么是词嵌入和它们的不同类型以及实际实现它们用来执行诸如返回有效的谷歌搜索结果的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. What are Word Embeddings?\n",
    "2. Different types of Word Embeddings\n",
    "\n",
    "    2.1 Frequency based Embedding\n",
    "        2.1.1 Count Vectors\n",
    "        2.1.2 TF-IDF\n",
    "        2.1.3 Co-Occurrence Matrix\n",
    "    2.2  Prediction based Embedding\n",
    "        2.2.1 CBOW\n",
    "        2.2.2 Skip-Gram\n",
    "3. Word Embeddings use case scenarios(what all can be done using word embeddings? eg: similarity, odd one out etc.)\n",
    "4. Using pre-trained Word Vectors\n",
    "5. Training your own Word Vectors\n",
    "6. End Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录表\n",
    "1. 什么是词嵌入？\n",
    "2. 词嵌入的不同类型\n",
    "    2.1 基于频率的词嵌入\n",
    "        2.1.1 计数向量\n",
    "        2.1.2 TF-IDF词频-逆文档词频\n",
    "        2.1.3 共现矩阵\n",
    "    2.2 基于预测的词嵌入\n",
    "        2.2.1 CBOW 连续词袋模型\n",
    "        2.2.2 Skip-Gram\n",
    "3. 词嵌入使用案例情景（词嵌入可以做什么？如：相似性、找不一样）\n",
    "4. 使用预训练词向量\n",
    "5. 训练你自己的词向量\n",
    "6. 结束语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are Word Embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 什么是词嵌入？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. But before we dive into the details of Word Embeddings, the following question should be asked – Why do we need Word Embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通俗来讲，词嵌入就是文本转换成数字，同样的文本可能有不同的数值表示。但是在我们深入词嵌入细节之前，应该先回答如下问题——为什么我们需要词嵌入？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. And with the huge amount of data that is present in the text format, it is imperative to extract knowledge out of it and build applications. Some real world applications of text applications are – sentiment analysis of reviews by Amazon etc., document or news classification or clustering by Google etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "许多机器学习算法和几乎所有的深度学习架构被证明不适合处理原始形式的字符串或文本。它们需要数字作为输入来执行任何分类、回归等任务。广而言之，随着海量文本数据涌现，从中抽取知识和构建应用迫在眉睫。一些真实世界的文本应用是——亚马逊公司做的评论情感分析，谷歌公司做的文档或新闻分类或聚类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define Word Embeddings formally. A Word Embedding format generally tries to map a word using a dictionary to a vector. Let us break this sentence down into finer details to have a clear view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们正式地定义词嵌入。一个词嵌入格式通常尝试把一个词使用字典映射到向量。让我们把下面这个句子分解成更好的细节，以便得到一个清晰的视图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary may be the list of all unique words in the sentence. So, a dictionary may look like – [‘Word’,’Embeddings’,’are’,’Converted’,’into’,’numbers’]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词典可以是句子中所有唯一单词构成的列表。因此，一个词典可能像——[‘Word’,’Embeddings’,’are’,’Converted’,’into’,’numbers’]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector representation of a word may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. The vector representation of “numbers” in this format according to the above dictionary is [0,0,0,0,0,1] and of converted is[0,0,0,1,0,0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词的向量表示可以是独热码向量，其中1代表该词所在的位置，而0表示其它位置。用这种格式根据上面的词典“numbers”的向量表示是[0,0,0,0,0,1]，而\"converted\"的向量表示为[0,0,0,1,0,0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a very simple method to represent a word in the vector form. Let us look at different types of Word Embeddings or Word Vectors and their advantages and disadvantages over the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这仅是一种非常简单的方法,用于把一个词表示为向量形式。让我们来看看不同类型的词嵌入或词向量和它们较其它类型的优缺点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Different types of Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 不同类型的词嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different types of word embeddings can be broadly classified into two categories-\n",
    "\n",
    "1. Frequency based Embedding\n",
    "2. Prediction based Embedding\n",
    "\n",
    "Let us try to understand each of these methods in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同类型的词嵌入可以笼统的划分为两类：\n",
    "1. 基于频率的词嵌入\n",
    "2. 基于预测的词嵌入\n",
    "\n",
    "让我们尝试仔细地理解其中的每一种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Frequency based Embedding\n",
    "There are generally three types of vectors that we encounter under this category.\n",
    "\n",
    "1. Count Vector\n",
    "2. TF-IDF Vector\n",
    "3. Co-Occurrence Vector\n",
    "\n",
    "Let us look into each of these vectorization methods in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这类别下通常会遇到三种类型的词向量。\n",
    "1. 计数向量\n",
    "2. TF-IDF向量\n",
    "3. 共现向量\n",
    "\n",
    "让我们详细的研究下其中每一种向量化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Count Vector\n",
    "Consider a Corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 计数向量\n",
    "考虑D个文档{d1,d2,...dD}组成的语料库C，从语料库C中抽取的N个唯一标记。这N个唯一标记将形成我们的词典，计数向量矩阵M的大小将由D×N给定。矩阵M中的每一行（i行）包含文档D(i)中唯一标记的频率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand this using a simple example.\n",
    "\n",
    "D1: He is a lazy boy. She is also lazy.\n",
    "\n",
    "D2: Neeraj is a lazy person.\n",
    "\n",
    "The dictionary created may be a list of unique tokens(words) in the corpus =[‘He’,’She’,’lazy’,’boy’,’Neeraj’,’person’]\n",
    "\n",
    "Here, D=2, N=6\n",
    "\n",
    "The count matrix M of size 2 X 6 will be represented as –\n",
    "\n",
    " |   | He |\tShe | lazy | boy | Neeraj | person |\n",
    " | -- | ---- | ---- | --- | ----- | ------ |\n",
    " | D1 |\t1 |\t1 |\t2 |\t1 |\t0 |\t0 |\n",
    " | D2 |\t0 |\t0 |\t1 |\t0 |\t1 |\t1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们使用一个简单的例子来理。\n",
    "\n",
    "D1: He is a lazy boy. She is also lazy.\n",
    "\n",
    "D2: Neeraj is a lazy person.\n",
    "\n",
    "创建的词典可以是语料库中唯一标记（词语）组成的列表\n",
    "\n",
    "['He','She','lazy','boy','Neeraj','person']\n",
    "\n",
    "这里，D=2，N=6\n",
    "\n",
    "矩阵2×6大小的计数矩阵呈现如下：\n",
    "\n",
    " |   | He |\tShe | lazy | boy | Neeraj | person |\n",
    " | -- |----| ----| -----|-----| ------ | ------ |\n",
    " | D1 |\t1 |\t1 |\t2 |\t1 |\t0 |\t0 |\n",
    " | D2 |\t0 |\t0 |\t1 |\t0 |\t1 |\t1 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a column can also be understood as word vector for the corresponding word in the matrix M. For example, the word vector for ‘lazy’ in the above matrix is [2,1] and so on.Here, the rows correspond to the documents in the corpus and the columns correspond to the tokens in the dictionary. The second row in the above matrix may be read as – D2 contains ‘lazy’: once, ‘Neeraj’: once and ‘person’ once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，在矩阵M中一列也可以理解为相应词的词向量。例如，在上面矩阵中的'lazy'一词的词向量是[2,1]等等。这里，行对应语料库中的文档，列对应词典中的标记。上面矩阵的第二行可以读作：D2包含'lazy'一次，'Neeraj'一次，'person'一次'。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there may be quite a few variations while preparing the above matrix M. The variations will be generally in-\n",
    "\n",
    "1. The way dictionary is prepared.\n",
    "\n",
    "Why? Because in real world applications we might have a corpus which contains millions of documents. And with millions of document, we can extract hundreds of millions of unique words. So basically, the matrix that will be prepared like above will be a very sparse one and inefficient for any computation. So an alternative to using every unique word as a dictionary element would be to pick say top 10,000 words based on frequency and then prepare a dictionary.\n",
    "2. The way count is taken for each word.\n",
    "\n",
    "We may either take the frequency (number of times a word has appeared in the document) or the presence(has the word appeared in the document?) to be the entry in the count matrix M. But generally, frequency method is preferred over the latter.\n",
    "Below is a representational image of the matrix M for easy understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，在准备上述矩阵M时可能会有相当多的变化。这些变化通常是：\n",
    "1. 准备字典的方式\n",
    "    \n",
    "为什么？在因为现实世界的应用程序中，我们可能有一个包含数十万文档的语料库。凭借数十万的文档，我们可以提取数以亿记的独特单词。所以基本上，如上所述准备的矩阵将是非常稀疏的并且对于任何计算都是低效的。因此，使用每个独特单词作为词典元素的替代方法是基于频率挑选前10000个单词然后准备词典。\n",
    "2. 每个单词的计数方式\n",
    "\n",
    "我们可以采用频率（一个词在文档中出现的次数）或者存在（一个词在文档中出现与否？）作为计数矩阵M的条目。但是一般地，频率方法优先于后者。下面是矩阵M的表示图，以便理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 TF-IDF vectorization\n",
    "This is another method which is based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. So, what is the rationale behind this? Let us try to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 TF-IDF 向量化\n",
    "这是另一种基于频率的方法，但是它不同于计数向量化，原因是它不仅考虑单词在单文档中的出现而且考虑其在整个语料库中出现。那么，这背后的原因是什么呢？让我们试着去理解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common words like ‘is’, ‘the’, ‘a’ etc. tend to appear quite frequently in comparison to the words which are important to a document. For example, a document A on Lionel Messi is going to contain more occurences of the word “Messi” in comparison to other documents. But common words like “the” etc. are also going to be present in higher frequency in almost every document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与对文档重要的单词相比，诸如'is','the','a'等常用词汇往往更频繁地出现。例如，在ionel Messi上的文档A相比其它文档将包含\"Messi\"更多的出现。但是，像'the'等的常用词汇在几乎每个文档中以更高的频率出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理想情况下，我们想要的是降低出现在几乎在所有中的常用单词的权重，并提高出现在文档子集中的单词的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF靠通过分配较低的权值惩罚这些常用单词，同时提高如'Messi'这样在特别的文档中出现的单词的重要程度来起作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how exactly does TF-IDF work?\n",
    "\n",
    "Consider the below sample table which gives the count of terms(tokens/words) in two documents.\n",
    "\n",
    "Now, let us define a few terms related to TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，TF-IDF到底如何工作的呢？\n",
    "考虑下面的例表，给出2个文档中条目（标记/单词）的数量。\n",
    "现在，让我们定义几个TF-IDF相关的术语。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "\n",
    "So, TF(This,Document1) = 1/8\n",
    "TF(This, Document2)=1/5\n",
    "\n",
    "It denotes the contribution of the word to the document i.e words relevant to the document should be frequent. eg: A document about Messi should contain the word ‘Messi’ in large number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF = (文档中条目t出现的次数)/(文档条目总数)\n",
    "\n",
    "那么，TF(Thisx，文档1) =1/8\n",
    "\n",
    "TF(This，文档2) =1/5\n",
    "\n",
    "它表示该词对文档的贡献，即与文档有关的词应该经常的出现。例如：一个关于梅西的文档应该包含大量的'Messi'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "\n",
    "where N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "\n",
    "So, IDF(This) = log(2/2) = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF = log(N/n),其中，N表示文档数量，n表示条目t出现过的文档数量。\n",
    "其中，N表示文档数量，n表示条目t出现过的文档数量。\n",
    "\n",
    "那么，IDF(This) = log(2/2) = 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us compare the TF-IDF for a common word ‘This’ and a word ‘Messi’ which seems to be of relevance to Document 1.\n",
    "\n",
    "TF-IDF(This,Document1) = (1/8) * (0) = 0\n",
    "\n",
    "TF-IDF(This, Document2) = (1/5) * (0) = 0\n",
    "\n",
    "TF-IDF(Messi, Document1) = (4/8)*0.301 = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们比较常用词'This'与看似与文档1相关的'Messi'的TF-IDF\n",
    "\n",
    "TF-IDF(This,文档1) = (1/8) * (0) = 0\n",
    "\n",
    "TF-IDF(This, 文档2) = (1/5) * (0) = 0\n",
    "\n",
    "TF-IDF(Messi, 文档1) = (4/8)*0.301 = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, you can see for Document1 , TF-IDF method heavily penalises the word ‘This’ but assigns greater weight to ‘Messi’. So, this may be understood as ‘Messi’ is an important word for Document1 from the context of the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如你看到的文档1，TF-IDF方法狠狠地惩罚了'This'这个词，但是分配给'Messi'更高的权重。那么，这可以理解为从整个语料库的上下文来看，‘Messi’是对文档1对更重要的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
